<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MAE on Jonathan`s Blog</title>
    <link>http://jonathanwayy.xyz/tags/mae/</link>
    <description>Recent content in MAE on Jonathan`s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>All rights reserved - 2020</copyright>
    <lastBuildDate>Fri, 17 Feb 2023 12:42:33 +0800</lastBuildDate><atom:link href="http://jonathanwayy.xyz/tags/mae/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[论文阅读笔记 -- MAE] Data Efficient Masked Language Modeling for VL (EMNLP 2021)</title>
      <link>http://jonathanwayy.xyz/2023/prn337/</link>
      <pubDate>Fri, 17 Feb 2023 12:42:33 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2023/prn337/</guid>
      <description>Data Efficient Masked Language Modeling for Vision and Language (EMNLP 2021) 开源代码传送门 概述 本文为 MLM 预训练提出新的掩码策略。</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- MAE] SimVTP: Simple Video Text Pre-training with MAEs (2022)</title>
      <link>http://jonathanwayy.xyz/2023/prn336/</link>
      <pubDate>Thu, 16 Feb 2023 21:26:17 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2023/prn336/</guid>
      <description>2212.03490 SimVTP: Simple Video Text Pre-training with Masked Autoencoders (2022) 开源代码传送门 概述 本文基于 MAE 提出一种简单视频-文本预训练框架 SimVTP。 模型架构 可视化示例</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- VLP] Learning Transferable Visual Models From NL Supervision (ICML 2021)</title>
      <link>http://jonathanwayy.xyz/2023/prn335/</link>
      <pubDate>Thu, 16 Feb 2023 17:37:17 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2023/prn335/</guid>
      <description>Learning Transferable Visual Models From Natural Language Supervision (ICML 2021) 开源代码传送门 概述 本文提出图文预训练模型 Contrastive Language-Image Pre-training (CLIP)。 模型架构 核心机制伪代码</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- MAE] Multi-modal Masked Autoencoders for Medical VLP (MICCAI 2022)</title>
      <link>http://jonathanwayy.xyz/2023/prn334/</link>
      <pubDate>Thu, 16 Feb 2023 17:03:21 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2023/prn334/</guid>
      <description>Multi-modal Masked Autoencoders for Medical Vision-and-Language Pre-training (MICCAI 2022) 开源代码传送门 概述 本文提出一种 Multi-modal Masked Autoencoder (M3AE) 架构。 模型架构 可视化示例</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- MAE] Masked Contrastive Pre-Training for Efficient VT Retrieval (2022)</title>
      <link>http://jonathanwayy.xyz/2023/prn333/</link>
      <pubDate>Thu, 16 Feb 2023 16:45:41 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2023/prn333/</guid>
      <description>2212.00986 Masked Contrastive Pre-Training for Efficient Video-Text Retrieval (2022) 开源代码传送门 概述 本文针对文本-视频预训练，提出一种 Masked Contrastive Pre-Training (MAC) 方法。 模型架构</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- MAE] VLMAE: Vision-Language Masked Autoencoder (2022)</title>
      <link>http://jonathanwayy.xyz/2023/prn332/</link>
      <pubDate>Thu, 16 Feb 2023 16:22:44 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2023/prn332/</guid>
      <description>2208.09374 VLMAE: Vision-Language Masked Autoencoder (2022) 概述 本文提出一种新的图文预训练框架，称为 Vision-Language Masked Autoencoder (VLMAE) 图文数据集情况 模型架构</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- MAE] Multimodal Masked AEs Learn Transferable Representations (2022)</title>
      <link>http://jonathanwayy.xyz/2023/prn331/</link>
      <pubDate>Thu, 16 Feb 2023 16:02:26 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2023/prn331/</guid>
      <description>2205.14204 Multimodal Masked Autoencoders Learn Transferable Representations (2022) 开源代码传送门 概述 本文提出 Multimodal Masked Autoencoders (M3AE)，为图文数据学习一个整合的模型。 核心想法为将一个图文对视为一个令牌长序列。 模型架</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- MAE] Masked Autoencoders Are Scalable Vision Learners (CVPR 2022)</title>
      <link>http://jonathanwayy.xyz/2023/prn330/</link>
      <pubDate>Thu, 16 Feb 2023 14:57:16 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2023/prn330/</guid>
      <description>Masked Autoencoders Are Scalable Vision Learners (CVPR 2022) 开源代码传送门 概述 将 MAE 引入到视觉任务。 模型架构 加掩码重构效果示例 掩码率的影响 掩码采样策略对比</description>
    </item>
    
  </channel>
</rss>

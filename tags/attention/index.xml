<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>attention on Jonathan`s Blog</title>
    <link>http://jonathanwayy.xyz/tags/attention/</link>
    <description>Recent content in attention on Jonathan`s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>All rights reserved - 2020</copyright>
    <lastBuildDate>Fri, 30 Jul 2021 11:12:23 +0800</lastBuildDate><atom:link href="http://jonathanwayy.xyz/tags/attention/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] Retrieve Fast Rerank Smart: Cooperative and Joint Approaches (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn64/</link>
      <pubDate>Fri, 30 Jul 2021 11:12:23 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn64/</guid>
      <description>2103.11920v1 Retrieve Fast, Rerank Smart: Cooperative and Joint Approaches for Improved Cross-Modal Retrieval (2021) 开源代码传送门 核心想法 要在计算效率和模型精度上找到平衡。 四种模型 Cross-Encoders 如图 1(a) 所示。 二分类问题，将 \([CLS]\) token 输入分类器，用交</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] TIED: A Cycle Consistent Encoder-Decoder Model (CVPRW 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn63/</link>
      <pubDate>Thu, 29 Jul 2021 11:27:32 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn63/</guid>
      <description>TIED: A Cycle Consistent Encoder-Decoder Model for Text-to-Image Retrieval (CVPRW 2021) 概览 Natural Language (NL) based Vehicle Track Retrieval 任务，对时间也有要求。 本文提出一种文本到图像的编码器解码器网络 (Text-to-Image Encoder-Decoder network, TIED)，将图文映射到潜在空间</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 联邦学习 / 跨模态检索] FedCMR: Federated Cross-Modal Retrieval (SIGIR 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn62/</link>
      <pubDate>Wed, 28 Jul 2021 14:36:28 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn62/</guid>
      <description>FedCMR: Federated Cross-Modal Retrieval (SIGIR 2021) 概述 基于深度学习的方法需要大量高质量的多模态数据，而现实中，多模态数据由许多不同用户 (client) 分别生成。 本文研究联邦跨模态检索 (Federated Cross-Modal Retrieval, Fe</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ViT / 跨模态检索] Revamping Cross-Modal Recipe Retrieval (CVPR 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn61/</link>
      <pubDate>Wed, 28 Jul 2021 10:41:00 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn61/</guid>
      <description>Revamping Cross-Modal Recipe Retrieval with Hierarchical Transformers and Self-supervised Learning (CVPR 2021) 开源代码传送门 图像编码器 \(\phi_{img}\) ResNet-50 / ResNeXt / ViT 菜谱编码器 \(\phi_{rec}\) 三类数据要处理 标题 成分 指导 用三个独立的基于 Transformer 的编码器分别处理三种数据</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ViT] Early Convolutions Help Transformers See Better (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn60/</link>
      <pubDate>Tue, 27 Jul 2021 11:07:09 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn60/</guid>
      <description>2106.14881 Early Convolutions Help Transformers See Better (2021) ViT 模型对超参数敏感，不好优化；而 CNN 更容易优化。 本文认为问题在于 ViT 的前期视觉处理 (early visual processing)，通常是 patchify stem，</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索 / 图网络] Fine-grained Video-Text Retrieval with HGR (CVPR 2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn59/</link>
      <pubDate>Mon, 26 Jul 2021 11:53:00 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn59/</guid>
      <description>Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning (CVPR 2020) 开源代码传送门 核心 提出一种层级式的图推理模型 (Hierarchical Graph Reasoning model, HGR)，将视频-文本匹配分解为三级语义： 全局事件，在文本中对应整个句</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索 / 图网络] A Deep Local and Global Scene-Graph Matching (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn58/</link>
      <pubDate>Sun, 25 Jul 2021 11:30:31 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn58/</guid>
      <description>2106.02400 A Deep Local and Global Scene-Graph Matching for Image-Text Retrieval (2021) 核心 提出一种局部与全局场景图匹配 (Local and Global Scene Graph Matching, LGSGM) 方法。 视觉图编码器 文本编码器 图嵌入 用多尺度节点注意力将图嵌入为向量。 附</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ViT / ReID] TransReID: Transformer-based Object Re-Identification (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn57/</link>
      <pubDate>Wed, 21 Jul 2021 10:10:54 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn57/</guid>
      <description>2102.04378v2 TransReID: Transformer-based Object Re-Identification (2021) 开源代码传送门 背景 目标 ReID 尚未很好解决的两个问题 从全局视角提取丰富的结构模式 包含细节信息的细粒度特征提取受到下采样的限制 本文提出一</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ViT] CMT: Convolutional Neural Networks Meet Vision Transformers (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn56/</link>
      <pubDate>Tue, 20 Jul 2021 14:32:30 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn56/</guid>
      <description>2107.06263 CMT: Convolutional Neural Networks Meet Vision Transformers (2021) 核心 本文设计了一种 CNN 与 transformer 的交集，即 CMT 架构。 CMT 块 Local Perception Unit (LPU) 绝对的位置编码破坏了平移不变性，忽视了 patch 中的局部关联与结构信息。 $$LPU(X) =</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ViT] Incorporating Convolution Designs into Visual Transformers (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn55/</link>
      <pubDate>Tue, 20 Jul 2021 10:06:20 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn55/</guid>
      <description>2103.11816 Incorporating Convolution Designs into Visual Transformers (2021) 核心 设计一种通过卷积增强的图像 Transformer (Convolution-enhanced image Transformer, CeiT)，将 CNN 提取低级特征、强化局部性与 Transformer 提取长程依赖的优势相结合。 Image-to-Tokens 模块 $$x&#39; = I2T(x) = MaxPool(BN(Conv(x))).$$</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] Heterogeneous Attention Network (SIGIR 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn54/</link>
      <pubDate>Sun, 18 Jul 2021 19:39:50 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn54/</guid>
      <description>Heterogeneous Attention Network for Effective and Efficient Cross-modal Retrieval (SIGIR 2021) 背景 联合嵌入 (joint embedding) 方法的问题 只进行全局匹配 文本与图像只在匹配阶段有交互 多模态的 Transformer 系方法能够在较早的阶段实现跨模态交互，但</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索 / 图网络] Cross-Graph Attention Model (SIGIR 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn53/</link>
      <pubDate>Sun, 18 Jul 2021 11:25:44 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn53/</guid>
      <description>Cross-Graph Attention Enhanced Multi-Modal Correlation Learning for Fine-Grained Image-Text Retrieval (SIGIR 2021) 背景 SIGIR 2021 短文。 现有三类跨模态检索方法 全局相关性学习 局部相关性学习 高阶语义概念学习 在模态特定的语义概念 (modality-specific semantic concepts) 之外，还应</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索 / 图网络] Graph Structured Network (CVPR 2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn52/</link>
      <pubDate>Sun, 18 Jul 2021 10:10:07 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn52/</guid>
      <description>2004.00277 Graph Structured Network for Image-Text Matching (CVPR 2020) 开源代码传送门 总览 本文提出一种图结构匹配网络 (Graph Structured Matching Network, GSMN)，对目标、关系与属性显式建模。 以下两组相关性互相促进 细粒度目</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 频域 / ViT] M2TR: Multi-modal Multi-scale Transformers (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn51/</link>
      <pubDate>Sat, 17 Jul 2021 14:34:02 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn51/</guid>
      <description>2104.09770 M2TR: Multi-modal Multi-scale Transformers for Deepfake Detection (2021) 背景 Deepfake 检测任务。 本文提出一种多模态多尺度 Transformer (Multi-modal Multi-scale Transformer, M2TR)，包含一个多尺度 Transformer 模块 (MT) 和一个跨模态融合模块 (CMF)，利用频域</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 频域] Global Filter Networks for Image Classification (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn49/</link>
      <pubDate>Fri, 16 Jul 2021 19:50:39 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn49/</guid>
      <description>2107.00645 Global Filter Networks for Image Classification (2021) 开源代码传送门 核心方法 本文提出一种全局滤波器网络 (Global Filter Network, GFNet)，在频域中学习空间位置之间的相互关系。 不同于视觉 transformer 中的自注</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] Multi-Modality Cross Attention Network (CVPR 2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn45/</link>
      <pubDate>Wed, 14 Jul 2021 10:42:26 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn45/</guid>
      <description>Multi-Modality Cross Attention Network for Image and Sentence Matching (CVPR 2020) 出发点 既考虑模态间关联，也考虑模态内关联。 提出多模态交叉注意力网络 (Multi-Modality Cross Attention Network)，主要由自注意力模块与交叉注意</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 注意力机制] Non-Local Neural Networks (CVPR 2018)</title>
      <link>http://jonathanwayy.xyz/2021/prn43/</link>
      <pubDate>Mon, 12 Jul 2021 15:09:26 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn43/</guid>
      <description>Non-Local Neural Networks (CVPR 2018) 开源代码传送门 核心内容 提出非局部操作用于捕捉长距离依赖关系，通过输入特征图所有位置上特征的加权和计算各位置的响应值 (respons</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] CAMP: Cross-Modal Adaptive Message Passing (ICCV 2019)</title>
      <link>http://jonathanwayy.xyz/2021/prn42/</link>
      <pubDate>Mon, 12 Jul 2021 10:58:13 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn42/</guid>
      <description>CAMP: Cross-Modal Adaptive Message Passing for Text-Image Retrieval (ICCV 2019) 开源代码传送门 背景 现有方法的问题 现有方法通常学习一个公共的嵌入空间，在其中衡量特征相似度，使用 ranking loss 进行训练。 这类方法没有</description>
    </item>
    
    <item>
      <title>炼丹杂记 -- Channel Shuffle 操作的 PyTorch 实现</title>
      <link>http://jonathanwayy.xyz/2021/channel-shuffle/</link>
      <pubDate>Sun, 11 Jul 2021 14:26:18 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/channel-shuffle/</guid>
      <description>在 ShuffleNet、SA-Net 以及一系列模型中涉及到了一种 Channel Shuffle 操作，用于在沿着通道维分组运算后保证各组特征之间能够有信息交互。 Channel Shuffle 的机</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 注意力机制] SA-Net: Shuffle Attention for Deep CNNs (ICASSP 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn40/</link>
      <pubDate>Sun, 11 Jul 2021 10:53:48 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn40/</guid>
      <description>2102.00240v1 SA-Net: Shuffle Attention for Deep Convolutional Neural Networks (ICASSP 2021) 开源代码传送门 背景 设计了 Shuffle Attention (SA) 模块，将特征沿着通道维分组，对每个子特征用 Shuffle 单元同时计算通道注意力与空间注意力。 Shuffle Attention (SA) 特</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 频域 / Transformer] FNet: Mixing Tokens with Fourier Transforms (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn38/</link>
      <pubDate>Sat, 10 Jul 2021 15:34:14 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn38/</guid>
      <description>2105.03824 FNet: Mixing Tokens with Fourier Transforms (2021) 开源代码传送门 出发点 用更简单的 token 混合机制取代自注意力层。 最终选择傅利叶变换，设计了 FNet 模型。 离散傅利叶变换 (Discrete Fourier Transform, DFT) 傅利叶变换将</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ViT / 跨模态检索] Fine-grained Visual Textual Alignment (2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn37/</link>
      <pubDate>Sat, 10 Jul 2021 11:14:29 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn37/</guid>
      <description>2008.05231 Fine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders (2020) 背景 在分类任务上预训练的 CNN 网络所提取的特征通常只能捕捉到图像的全局描述，而忽视了重要的局部细节。 现有方法的问题 由于交</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ViT] Swin Transformer (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn35/</link>
      <pubDate>Fri, 09 Jul 2021 10:54:19 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn35/</guid>
      <description>2103.14030 Swin Transformer: Hierarchical Vision Transformer using Shifted Windows (2021) 开源代码传送门 背景 Transformer 在视觉任务上的主要困难 视觉元素的尺度可能相当不同，但是当前工作中 token 都固定尺度 图像具有更高的像素分辨率</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ViT] XCiT: Cross-Covariance Image Transformers (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn32/</link>
      <pubDate>Mon, 05 Jul 2021 12:59:51 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn32/</guid>
      <description>2106.09681 XCiT: Cross-Covariance Image Transformers (2021) 开源代码传送门 背景 Transformers 中自注意力模块计算复杂度高。 本文用一种转置的注意力 (transposed attention) 取代自注意力，称为交叉协方差注意力 (cross-covariance attention, XCA)，其对于</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 注意力机制] VOLO: Vision Outlooker for Visual Recognition (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn30/</link>
      <pubDate>Sat, 03 Jul 2021 16:23:02 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn30/</guid>
      <description>2106.13112v2 VOLO: Vision Outlooker for Visual Recognition (2021) 背景 制约 ViT 不如 CNN 的一个主要因素 ViT 在将细粒度特征以及上下文编码成 token 时效率较低。 本文设计了一种简单轻量的注意力机制，称为 Outl</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 频域] FcaNet: Frequency Channel Attention Networks (2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn13/</link>
      <pubDate>Sun, 20 Jun 2021 14:25:10 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn13/</guid>
      <description>2012.11879 FcaNet: Frequency Channel Attention Networks (2020) 利用频率分析重新思索通道注意力，并从数学上证明传统的全局平均池化 (GAP) 是频域中特征分解的一种特殊情况。 GAP 的潜在问题 尽管简洁高效， GAP</description>
    </item>
    
  </channel>
</rss>

<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>paper reading on Jonathan`s Blog</title>
    <link>http://jonathanwayy.xyz/tags/paper-reading/</link>
    <description>Recent content in paper reading on Jonathan`s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>All rights reserved - 2020</copyright>
    <lastBuildDate>Fri, 26 Nov 2021 13:08:31 +0800</lastBuildDate><atom:link href="http://jonathanwayy.xyz/tags/paper-reading/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[论文阅读笔记 -- ReID] CMReID via Modality Confusion and Center Aggregation(ICCV 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn124/</link>
      <pubDate>Fri, 26 Nov 2021 13:08:31 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn124/</guid>
      <description>Cross-Modality Person Re-Identification via Modality Confusion and Center Aggregation (ICCV 2021) 概述 本文提出一种端到端的模态混淆学习网络 (Modality Confusion Learning network, MCLNet)，其核心想法在于混淆特征学习过程中的模态鉴别，使得优化显</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ReID] SphereReID: Deep Hypersphere Manifold Embedding for ReID (2019)</title>
      <link>http://jonathanwayy.xyz/2021/prn123/</link>
      <pubDate>Thu, 25 Nov 2021 10:10:27 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn123/</guid>
      <description>SphereReID: Deep Hypersphere Manifold Embedding for Person Re-identification (2019) 开源代码传送门 概述 本文提出一种 metric-based 的架构，称为 SphereReID，引入一个新的损失函数 Sphere Loss。 Softmax Loss $$L_{softmax} = -\frac{1}{N} \sum_{i= 1}^{N} log \frac{e^{z_{y_{i}}}}{\sum_{j=1}^C e^{e^{z_{j}}}},$$ $$z_{j} =</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ReID] GreyReID: Two-stream Framework with RGB-grey Information (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn122/</link>
      <pubDate>Wed, 24 Nov 2021 12:41:55 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn122/</guid>
      <description>GreyReID: A Novel Two-stream Deep Framework with RGB-grey Information for Person Re-identification (TOMM 2021) 概述 着重关注不同行人之间色彩信息相似的问题，本文称之为 ReID 的色彩过拟合 (color over-fitting)。 RGB 图像与灰度图</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ReID / 频域] HLFNet: High-low Frequency Network for Person ReID (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn121/</link>
      <pubDate>Tue, 23 Nov 2021 14:29:09 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn121/</guid>
      <description>HLFNet: High-low Frequency Network for Person Re-Identification (IEEE Signal Processing Letters 2021) 概述 本文提出一种高低频网络 (high-low frequency network, HLFNet)。 Frequency Splitting Module (FSM) 利用 guide filter 将原始图像分为高低频图像： $$I_{l} = \mathcal{G}(I_{o}),$$ $$I_{h} = I_{o} / (I_{l} + eps).$$ 将得到</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ReID] Occlude Them All: Occlusion-Aware Attention Network (ICCV 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn120/</link>
      <pubDate>Mon, 22 Nov 2021 15:13:24 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn120/</guid>
      <description>Occlude Them All: Occlusion-Aware Attention Network for Occluded Person Re-ID (ICCV 2021) 概述 将遮挡按如下分类 4 locations: top, bottom, left, right 2 areas: half, quarter 本文提出一种遮挡可知的掩码网络 (Occlusion-Aware Mask Network, OAMN)，包含三个主要部件： attention-guided mask module occlusion augmentation</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ReID] Occluded ReID With Single-Scale Global Representation (ICCV 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn119/</link>
      <pubDate>Sat, 20 Nov 2021 17:40:34 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn119/</guid>
      <description>Occluded Person Re-Identification With Single-Scale Global Representations (ICCV 2021) 数据集传送门（尚未更新） 概述 本文提出一种新的 ReID 模型，学习单尺度全局级别的行人表征 (single-scale global-level pedestrian representations)。 构</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ReID] Weakly Supervised Text-Based Person Re-Identification (ICCV 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn118/</link>
      <pubDate>Fri, 19 Nov 2021 16:42:59 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn118/</guid>
      <description>Weakly Supervised Text-Based Person Re-Identification (ICCV 2021) 开源代码传送门 概述 本文提出弱监督图文 ReID，即在训练阶段没有 ID 标注。 新任务的两个困难 各模态内由于类内差异造成的影响难以处理 跨</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ReID] CM-NAS for Visible-Infrared Person Re-Identification (ICCV 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn117/</link>
      <pubDate>Thu, 18 Nov 2021 09:42:55 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn117/</guid>
      <description>CM-NAS: Cross-Modality Neural Architecture Search for Visible-Infrared Person Re-Identification (ICCV 2021) 开源代码传送门 概述 Visible-Infrared ReID 任务。 现有工作多是设计一个 two-stream 架构，因而就产生了一个问题：哪些层应当被分为两个分支，哪些层应该共享</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ReID] BV-Person: A Large-Scale Dataset for Bird-View ReID (ICCV 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn116/</link>
      <pubDate>Wed, 17 Nov 2021 20:40:52 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn116/</guid>
      <description>BV-Person: A Large-Scale Dataset for Bird-View Person Re-Identification (ICCV 2021) 数据集与开源代码传送门 概述 本文提出一种新的 ReID 任务，即鸟瞰视角下的 ReID，并制作了一个大规模数据集，称为 BV-Perso</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ReID] LapsCore: Language-Guided Search via Color Reasoning (ICCV 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn115/</link>
      <pubDate>Mon, 15 Nov 2021 11:06:37 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn115/</guid>
      <description>LapsCore: Language-Guided Person Search via Color Reasoning (ICCV 2021) 概述 现有方法隐式地学习跨模态局部关联。 颜色在检索中至关重要。 本文提出一种基于颜色推理的新方法，称为 LapsCore，通过解</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ViT] Escaping the Big Data Paradigm with Compact Transformers (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn114/</link>
      <pubDate>Thu, 11 Nov 2021 20:39:25 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn114/</guid>
      <description>2104.05704 Escaping the Big Data Paradigm with Compact Transformers (2021) 开源代码传送门 概述 针对 Transformer 模型的数据饥饿 (data hungry) 传统观点，尝试弥合 Transformer 和 CNN 两种架构之间的鸿沟，结合二者优势，使得基于 Transformer 的模型能够</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] Structured MM Feature Embedding and Alignment (MM 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn113/</link>
      <pubDate>Wed, 10 Nov 2021 19:27:01 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn113/</guid>
      <description>Structured Multi-modal Feature Embedding and Alignment for Image-Sentence Retrieval (MM 2021) 概述 现有细粒度方法的问题 忽略了模态内的上下文语义以及部件之间的结构化关系，导致无法有效捕获语义 以多对多匹配范式隐式建模</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ReID] Joint Generative and Contrastive Learning for UnS. ReID (CVPR 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn112/</link>
      <pubDate>Tue, 09 Nov 2021 09:03:20 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn112/</guid>
      <description>Joint Generative and Contrastive Learning for Unsupervised Person Re-identification (CVPR 2021) 开源代码传送门 概述 自监督对比学习方法 对于一张图像，最大化其两个增广视角之间的共识 (agreement between two augmented views)，视角指的是对于某</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ReID] Query-Adaptive Convolution and Temporal Lifting (ECCV 2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn111/</link>
      <pubDate>Mon, 08 Nov 2021 12:14:22 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn111/</guid>
      <description>Interpretable and Generalizable Person Re-Identification with Query-Adaptive Convolution and Temporal Lifting (ECCV 2020) 开源代码传送门 概述 现有的许多 ReID 方法只是在两个表征向量之间简单计算距离，而无视了两张图像真实内容之间的直接关系。 本文</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ReID] Knowledge-SV Learning: Knowledge Consensus Constraints (MM 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn110/</link>
      <pubDate>Thu, 04 Nov 2021 11:07:28 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn110/</guid>
      <description>Knowledge-Supervised Learning: Knowledge Consensus Constraints for Person Re-Identification (MM 2021) 概述 本文旨在利用知识在不引入额外推断成本的基础上，约束同一数据上的多视角共识以提升精度。 行人 ReID 相较于图像分类的特殊性 检索</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] MCCN: Multimodal Coordinated Clustering Network (MM 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn109/</link>
      <pubDate>Tue, 02 Nov 2021 19:54:04 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn109/</guid>
      <description>MCCN: Multimodal Coordinated Clustering Network for Large-Scale Cross-modal Retrieval (MM 2021) 概述 本文关注大规模多个模态应用场景下的模态不平衡的跨模态检索问题，提出一种多模态协同的聚类网络 (Multimodal Coordinated Clustering Network, MCCN)。 MCCN 包</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ReID] Pose-guided Inter- and Intra-part Relational Transformer (MM 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn108/</link>
      <pubDate>Sun, 31 Oct 2021 17:14:50 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn108/</guid>
      <description>Pose-guided Inter- and Intra-part Relational Transformer for Occluded Person Re-Identification (MM 2021) 开源代码传送门 概述 本文提出一种姿态指导的部件内间关系 Transformer。 姿态指导的特征提取 $$M = GMP(\hat{M}),$$ $$c_{g} = max(M_{g}),$$ $$F_{pose} = GAP((M &amp;gt; \tau)</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 数据增强] Cut-Thumbnail: A Novel Data Augmentation for CNN (MM 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn107/</link>
      <pubDate>Fri, 29 Oct 2021 14:34:34 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn107/</guid>
      <description>Cut-Thumbnail: A Novel Data Augmentation for Convolutional Neural Network (MM 2021) 开源代码传送门 概述 现有的数据增强方法通过改变空间或色彩信息、增加噪音或混合来自不同图像的信息，来提高网络的泛化能力和鲁</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ReID] Pose-Guided Feature Learning with KD for Occluded ReID (MM 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn106/</link>
      <pubDate>Tue, 26 Oct 2021 22:45:19 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn106/</guid>
      <description>Pose-Guided Feature Learning with Knowledge Distillation for Occluded Person Re-Identification (MM 2021) 概述 本文提出一种通过知识蒸馏进行基于姿态指导的特征学习架构 (Pose-Guided Feature Learning with Knowledge Distillation network, PGFL-KD) 架构，姿态信息用于约束全局特征的学习而在测</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ReID] TBPS in Full Images via Semantic-Driven Proposal Generation (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn105/</link>
      <pubDate>Fri, 15 Oct 2021 16:42:40 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn105/</guid>
      <description>2109.12965 Text-based Person Search in Full Images via Semantic-Driven Proposal Generation 概述 提出在完整图像中进行行人检索的任务，可视为 Person Detection 和 Text-based Person Retrieval 的结合。 本文提出 Semantic-Driven Region Proposal Net (SDPRN)。 构建了两个新数据集 CUHK-SYSU-TBPS</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 图像分解] Blind Image Decomposition (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn104/</link>
      <pubDate>Sun, 26 Sep 2021 17:07:17 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn104/</guid>
      <description>2108.11364 Blind Image Decomposition (2021) 开源代码传送门 概述 本文提出提出盲图像分解 (Blind Image Decomposition, BID) 任务，并设计了 Blind Image Decomposition Network (BIDeN)。 BID 的特点 不固定源部件数目，只设定一个潜在源部</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ReID] Learning Posterior and Prior for Uncertainty Modeling in ReID(2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn103/</link>
      <pubDate>Sat, 25 Sep 2021 15:30:28 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn103/</guid>
      <description>2007.08785 Learning Posterior and Prior for Uncertainty Modeling in Person Re-Identification (2020) 概述 本文在 ReID 任务中同时学习样本后验与类别先验，以量化输入图像及其相应类别的不确定性。 整体架构 上分支用 GAP 处理特征图得到</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ReID] Person Search Challenges and Solutions: A Survey (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn102/</link>
      <pubDate>Fri, 24 Sep 2021 22:10:19 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn102/</guid>
      <description>2105.01605v1 Person Search Challenges and Solutions: A Survey (2021) 概述 关于 Image-based Person Search 与 Text-based Person Search 的综述。 本文与现有综述的差异 Detection-identification Inconsistency Problem Person Search 研究进展时间线 三个主要挑战 从场景图像学习具有足够鉴别能力的行人</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 人机交互 / 跨模态检索] Interactive Natural Language Person Search (2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn101/</link>
      <pubDate>Fri, 24 Sep 2021 19:53:50 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn101/</guid>
      <description>2002.08434v1 Interactive Natural Language-based Person Search (2020) CUHK-QA 数据集传送门 概述 可以认为本文是换了一种说法，把 Text-based Person Search 称为 Zero-shot re-ID。 将 language-based re-ID 视为一种 VQA 任务，输入为图文对，输出为二值化答案。</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 风格迁移] Style Transfer with Adaptive Instance Normalization (ICCV 2017)</title>
      <link>http://jonathanwayy.xyz/2021/prn100/</link>
      <pubDate>Thu, 23 Sep 2021 16:48:51 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn100/</guid>
      <description>Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization (ICCV 2017) 概述 本文基于 instance normalization (IN) 提出 adaptive instance normalization (AdaIN)，解决风格迁移任务中的灵活性-速度困境。 Adaptive Instance Normalization (AdaIN) $$AdaIN(x, y) = \sigma (y) (\frac{x - \mu (x)}{\sigma (x)}) + \mu (y).$$ 风</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 图像标注] Unsupervised Image Captioning (CVPR 2019)</title>
      <link>http://jonathanwayy.xyz/2021/prn99/</link>
      <pubDate>Wed, 22 Sep 2021 13:14:16 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn99/</guid>
      <description>Unsupervised Image Captioning (CVPR 2019) 开源代码传送门 概述 无监督图像标注任务，即不使用任何标记好的图文对来训练图像标注模型。 三个目标 用文本对抗生成方法，在句子语料上训练一</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态哈希] SDMCH: Supervised Discrete Manifold-Embeded (IJCAI 2018)</title>
      <link>http://jonathanwayy.xyz/2021/prn98/</link>
      <pubDate>Thu, 16 Sep 2021 15:23:30 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn98/</guid>
      <description>SDMCH: Supervised Discrete Manifold-Embedded Cross-Modal Hashing (IJCAI 2018) 概述 本文提出离散流形嵌入跨模态哈希方法 (Discrete Manifold-Embedded Cross-Modal Hashing, SDMCH)。 既挖掘数据的非线性流形结构，也在多模态之间构建相关性。 流形结构学</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] Semantically Self-Aligned Network for T2I Person ReID (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn97/</link>
      <pubDate>Mon, 13 Sep 2021 13:18:31 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn97/</guid>
      <description>2107.12666v2 Semantically Self-Aligned Network for Text-to-Image Part-aware Person Re-identification (2021) 开源代码传送门 概述 自然语言描述的自由形式带来的两个问题 同一张图像对应的描述可能非常不同 对于身体部件的描述可能以任意顺序呈</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ReID] Parameter-Efficient Person Re-identification in the 3D Space (2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn96/</link>
      <pubDate>Sat, 11 Sep 2021 14:44:37 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn96/</guid>
      <description>2006.04569 Parameter-Efficient Person Re-identification in the 3D Space (2020) 开源代码传送门 概述 考察 2D 行人外观与 3D 几何结构之间的互补信息。 本文提出 Omni-scale Graph Network (OG-Net)，在 3D 空间中进行 ReID。 模型架</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ReID] PCNET: Parallelly Conquer Large Variance of Person ReId (ICIP 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn95/</link>
      <pubDate>Fri, 10 Sep 2021 13:22:48 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn95/</guid>
      <description>PCNET: Parallelly Conquer the Large Variance of Person Re-Identification (ICIP 2021) 概述 本文提出 Parallelly Conquer Net (PCNet)，主要包含三个部件： Pose Adaptation Module (PAM) Global Alignment Module (GAM) Pixel-Wised Attention Module (PWAM) 用模块聚合单元 (Module Aggregation Unit) 整合各个子模块生成的特</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 3D 跨模态检索] Cross-Modal Center Loss for 3D CM Retrieval (CVPR 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn94/</link>
      <pubDate>Thu, 09 Sep 2021 22:52:53 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn94/</guid>
      <description>Cross-Modal Center Loss for 3D Cross-Modal Retrieval (CVPR 2021) 概述 现有方法的问题 核心想法是最小化由预训练模型提取的特征之间的跨模态差异，预训练模型没有参与训练或进行微调 现有的损失函数主</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] Modality-Specific and Shared GAN (PR 2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn93/</link>
      <pubDate>Thu, 09 Sep 2021 21:42:02 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn93/</guid>
      <description>Modality-Specific and Shared Generative Adversarial Network for Cross-modal Retrieval (PR 2020) 概述 本文提出 Modality-Specific and Shared Generative Adversarial Network (\(MS^2GAN\))。 生成模型 标签预测 将模态中特征与公共空间特征拼接后用于预测标签，采用</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态哈希] Aggregation-based Graph Convolutional Hashing (TMM 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn92/</link>
      <pubDate>Thu, 09 Sep 2021 13:11:23 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn92/</guid>
      <description>Aggregation-based Graph Convolutional Hashing for Unsupervised Cross-modal Retrieval (TMM 2021) 概述 本文提出基于聚合的图卷积哈希方法 (Aggregation-based Graph Convolutional Hashing, AGCH)。 模型架构 主要部件 图像编码器 + 图像 GCN 文本编码器 + 文本 GCN 融合模块 生成</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] Learning Sufficient Scene Representation(Neurocomputing 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn91/</link>
      <pubDate>Wed, 08 Sep 2021 19:36:34 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn91/</guid>
      <description>Learning Sufficient Scene Representation for Unsupervised Cross-modal Retrieval (Neurocomputing 2021) 背景 此前有工作从统计层面证明分析了跨模态检索的过程，借助变分推断证明了不可能同时最大化模态内与模态间相似度，二者会互相约</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] Unsupervised Cross-modal Retrieval through AL (ICME 2017)</title>
      <link>http://jonathanwayy.xyz/2021/prn90/</link>
      <pubDate>Tue, 07 Sep 2021 14:45:21 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn90/</guid>
      <description>Unsupervised Cross-modal Retrieval through Adversarial Learning (ICME 2017) 概述 本文提出基于对抗学习的无监督跨模态检索 (Unsupervised Cross-modal Retrieval with Adversarial, UCAL)。 包含四个部分： 图像特征映射 文本特征映射 模态分类器，生成二元特</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ViT] Contextual Transformer Networks for Visual Recognition (CVPRW 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn89/</link>
      <pubDate>Thu, 02 Sep 2021 21:44:32 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn89/</guid>
      <description>2107.12292 Contextual Transformer Networks for Visual Recognition (CVPRW 2021) 开源代码传送门 概述 现有 ViT 方法的问题 只是基于各位置上孤立的 key 和 query 求得注意力矩阵，忽略了相邻 key 之间丰富的上下文信息。 本文提出一</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态哈希] Attention-Guided Semantic Hashing (ICME 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn88/</link>
      <pubDate>Thu, 02 Sep 2021 12:29:35 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn88/</guid>
      <description>Attention-Guided Semantic Hashing for Unsupervised Cross-Modal Retrieval (ICME 2021) 概述 无监督跨模态哈希问题。 本文提出注意力指导的语义哈希模型 (Attention-Guided Semantic Hashing)。 模型架构 特征提取 VGG-16 提取图像特征，unive</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 频域] Learning Omni-frequency Region-adaptive Representations (AAAI 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn87/</link>
      <pubDate>Mon, 30 Aug 2021 11:13:40 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn87/</guid>
      <description>Learning Omni-frequency Region-adaptive Representations for Real Image Super-Resolution (AAAI 2021) 概述 本文提出 Omni-frequency Region-adaptive Network (ORNet) 解决真实图像超分问题。 模型架构 频率分解模块 (Frequency Decomposition Module, FD) FD 模块由两个阶段组成： 频率分解阶段 频率增强阶段 频率</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ReID] Learn 3D Shape Feature for Texture-insensitive Person ReID (CVPR 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn86/</link>
      <pubDate>Sun, 29 Aug 2021 14:30:30 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn86/</guid>
      <description>Learning 3D Shape Feature for Texture-insensitive Person Re-identification (CVPR 2021) 开源代码传送门 背景 Person ReID 任务。 研究表明 Person ReID 相当依赖衣着外观纹理 (clothing appearance textures)，大多数现有方法在衣着纹理较迷惑时表现</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 频域] Frequency Separation for Real-World Super-Resolution (ICCVW 2019)</title>
      <link>http://jonathanwayy.xyz/2021/prn85/</link>
      <pubDate>Fri, 27 Aug 2021 13:40:16 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn85/</guid>
      <description>1911.07850 Frequency Separation for Real-World Super-Resolution (ICCVW 2019) 开源代码传送门 速览 速读一篇文献，主要关注一下频域相关的处理。 下采样操作去除了高频信息而保留低频信息。 因而本文对高低频信息进行</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 频域] Focal Frequency Loss for Image Reconstruction (ICCV 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn84/</link>
      <pubDate>Thu, 26 Aug 2021 20:50:08 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn84/</guid>
      <description>2012.12821 Focal Frequency Loss for Image Reconstruction and Synthesis (ICCV 2021) 开源代码传送门 项目主页传送门 背景 Image Reconstruction And Synthesis 任务。 本文提出一种 Focal Frequency Loss，直接在频域中优化生成模型。 Focal Frequency Loss 由 2D DFT 的公式表</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] HAL: Mitigating Visual Semantic Hubs (AAAI 2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn83/</link>
      <pubDate>Thu, 26 Aug 2021 12:55:43 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn83/</guid>
      <description>HAL: Improved Text-Image Matching by Mitigating Visual Semantic Hubs (AAAI 2020) 背景 本文主要针对图文匹配任务中的枢纽点问题 (Hubness Problem) 进行研究。 由于图文匹配中的嵌入空间是由联合建模视觉和语言得到的，通常将其</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] Step-Wise Hierarchical Alignment Network (IJCAI 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn82/</link>
      <pubDate>Mon, 23 Aug 2021 14:43:44 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn82/</guid>
      <description>Step-Wise Hierarchical Alignment Network for Image-Text Matching (IJCAI 2021) 背景 现有方法的问题 根据明显差异来鉴别图文对，可能会因而无法区分具有微小上下文信息差异但是语义内容相似的样本。 本文提出一种逐</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] Probabilistic Embeddings for Cross-Modal Retrieval (CVPR 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn81/</link>
      <pubDate>Thu, 19 Aug 2021 11:43:18 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn81/</guid>
      <description>Probabilistic Embeddings for Cross-Modal Retrieval (CVPR 2021) 开源代码传送门 背景 一张图像可能与多条不同的文本相匹配，一条文本也可能对应多张不同的图像。 本文提出一种概率跨模态嵌入模型 (Probabilistic Cross-Modal Embedding, P</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 图网络 / Ad-hoc 检索] A Graph-based Relevance Matching Model (AAAI 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn80/</link>
      <pubDate>Tue, 17 Aug 2021 15:53:21 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn80/</guid>
      <description>2101.11873 A Graph-based Relevance Matching Model for Ad-hoc Retrieval (AAAI 2021) 背景 query-document 检索任务。 两类架构 representation-based matching interaction-based matching 两种重要关系 term-level query-document interaction document-level word relationships 一个问题 用来检索的短语可能并不连续出现在文档中，可能相隔甚远。</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 频域] FDA: Fourier Domain Adaptation (CVPR 2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn79/</link>
      <pubDate>Mon, 16 Aug 2021 16:19:12 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn79/</guid>
      <description>FDA: Fourier Domain Adaptation for Semantic Segmentation (CVPR 2020) 开源代码传送门 概述 提出傅里叶域自适应 (FDA)。 FFT ==&amp;gt; 用目标图像的低频成分替换源图像低频成分 ==&amp;gt; iFFT FDA 设计了一个外圈置零的掩码：</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 频域] Phase Consistent Ecological Domain Adaptation (CVPR 2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn78/</link>
      <pubDate>Sun, 15 Aug 2021 12:28:20 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn78/</guid>
      <description>Phase Consistent Ecological Domain Adaptation (CVPR 2020) 开源代码传送门 概述 无监督域适应 (Unsupervised Domain Adaptation, UDA) 任务。 通常的 UDA 方法 学习一个从源分布到目标分布的映射 训练一个对域变化不敏感的骨架网络 本文引</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ViT] CAT: Cross Attention in Vision Transformer (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn77/</link>
      <pubDate>Thu, 12 Aug 2021 16:00:40 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn77/</guid>
      <description>2106.05786 CAT: Cross Attention in Vision Transformer (2021) 开源代码传送门 核心 设计了一种在单通道特征图上做注意力的方法，提出交叉注意力。 结合 Transformer 和 CNN 的优点构建 CAT。 Patch 内自注意力块 (IPSA) 与 Patch</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 图网络] Graph Reasoning Networks on a Similarity Pyramid (ICCV 2019)</title>
      <link>http://jonathanwayy.xyz/2021/prn76/</link>
      <pubDate>Wed, 11 Aug 2021 15:46:56 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn76/</guid>
      <description>Fashion Retrieval via Graph Reasoning Networks on a Similarity Pyramid (ICCV 2019) 概述 Fashion Retrieval 任务。 一个问题在于对应位置的局部块通常是失配的，因此需要在同尺度所有局部之间遍历匹配。 设计了一种基于相似度金</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 注意力机制] Attention in Attention Network for Image Super-Resolution (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn75/</link>
      <pubDate>Tue, 10 Aug 2021 11:12:17 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn75/</guid>
      <description>2104.09497 Attention in Attention Network for Image Super-Resolution (2021) 开源代码传送门 概览 图像超分任务。 提出一种 attention in attention 块 (\(A^2B\))，并设计了 \(A^2N\) 架构。 出发点 两个问题 图像的哪一部分倾向于有</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 注意力机制] CCNet: Criss-Cross Attention (ICCV 2019)</title>
      <link>http://jonathanwayy.xyz/2021/prn74/</link>
      <pubDate>Mon, 09 Aug 2021 12:11:40 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn74/</guid>
      <description>CCNet: Criss-Cross Attention for Semantic Segmentation (ICCV 2019) 开源代码传送门 概述 提出十字注意力 (Criss-Cross Attention)，将参数量从非局部注意力的 \(H \times W\) 减少到了 \(H + W -1\)。 为了捕捉全图依</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] Learning Joint Embedding of Food Images and Recipes (TMM 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn73/</link>
      <pubDate>Sun, 08 Aug 2021 20:31:27 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn73/</guid>
      <description>Cross-Modal Food Retrieval: Learning a Joint Embedding of Food Images and Recipes with Semantic Consistency and Attention Mechanism (TMM 2021) 核心 相同食物的数据表征可能不同，而不同食物的数据表征可能相似，从而导致食物数据有较大的类内方差与较小</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 注意力机制] Attentional Feature Fusion (WACV 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn72/</link>
      <pubDate>Fri, 06 Aug 2021 14:50:11 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn72/</guid>
      <description>2009.14082 Attentional Feature Fusion (WACV 2021) 开源代码传送门 概述 本文研究特征融合 (feature fusion)，提出注意力特征融合模块 (attentional feature fusion module, AFF)。 为了缓解由于尺度变化以及小目标引起的</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] Context-Aware Attention Network (CVPR 2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn71/</link>
      <pubDate>Wed, 04 Aug 2021 19:43:07 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn71/</guid>
      <description>Context-Aware Attention Network for Image-Text Retrieval (CVPR 2020) 背景 现有方法的问题 不同的局部块有不同重要性 忽略同模态中各局部之间的语义相关性 一个单词或一个图像区域在不同的全局上下文中可能有</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ViT] Transformer in Convolutional Neural Networks (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn70/</link>
      <pubDate>Tue, 03 Aug 2021 12:09:03 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn70/</guid>
      <description>2106.03180 Transformer in Convolutional Neural Networks (2021) 开源代码传送门 概览 本文提出层级化的多头注意力 (H-MHSA)。 为了结合 CNN 与 Transformer 的优势，提出 Transformer in Convolutional Neural Networks (TransCNN) 的概念，TransCNN 直</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态哈希 / 图网络] Determining the Semantic Graph Connectivity (IJCAI 2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn69/</link>
      <pubDate>Tue, 03 Aug 2021 11:07:47 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn69/</guid>
      <description>Set and Rebase: Determining the Semantic Graph Connectivity for Unsupervised Cross-Modal Hashing (IJCAI 2020) 概述 两类无监督跨模态哈系 跨模态量化，最小化二进制编码与原始数据低维投影之间的鸿沟 跨模态相似度搜索 三个主要问题 在没</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索 / 图网络] Adapted Graph Reasoning and Filtration (SIGIR 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn68/</link>
      <pubDate>Mon, 02 Aug 2021 16:52:49 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn68/</guid>
      <description>Adapted Graph Reasoning and Filtration for Description-Image Retrieval (SIGIR 2021) 概述 本文处理更加抽象的文本描述。 提出一种自适应的图推理与过滤网络 (Adapted Graph Reasoning and Filtration network, AGRF)，包含两大主要部件： 自适应图推理网</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 频域] Wavelet Pooling for Convolutional Neural Networks (ICLR 2018)</title>
      <link>http://jonathanwayy.xyz/2021/prn67/</link>
      <pubDate>Sat, 31 Jul 2021 19:43:38 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn67/</guid>
      <description>Wavelet Pooling for Convolutional Neural Networks (ICLR 2018) 概述 本文提出一种小波池化算法，使用二阶小波分解对特征进行子采样，放弃了最近临插值方法，而采用了一种子带方法，从而得以使用更少</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 频域] Wavelet-enhanced Convolutional Neural Network (2018)</title>
      <link>http://jonathanwayy.xyz/2021/prn66/</link>
      <pubDate>Fri, 30 Jul 2021 21:55:30 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn66/</guid>
      <description>Wavelet-enhanced Convolutional Neural Network: A New Idea in A Deep Learning Paradigm (2018) 概览 结合 CNN 与小波变换提升分割任务表现。 小波变换在图像处理中的主要作用在于其能够将图像分解为含有不同级别细节的不同尺</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 频域] Wavelet Integrated CNNs for Noise-Robust Img Classification(CVPR 2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn65/</link>
      <pubDate>Fri, 30 Jul 2021 19:49:00 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn65/</guid>
      <description>Wavelet Integrated CNNs for Noise-Robust Image Classification (CVPR 2020) 开源代码传送门 概述 CNN 对噪声的鲁棒性较差，随机噪声大多为高频成分，CNN 在下采样之前缺少滤波步骤，可能会导致高低频成分的混叠</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] Retrieve Fast Rerank Smart: Cooperative and Joint Approaches (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn64/</link>
      <pubDate>Fri, 30 Jul 2021 11:12:23 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn64/</guid>
      <description>2103.11920v1 Retrieve Fast, Rerank Smart: Cooperative and Joint Approaches for Improved Cross-Modal Retrieval (2021) 开源代码传送门 核心想法 要在计算效率和模型精度上找到平衡。 四种模型 Cross-Encoders 如图 1(a) 所示。 二分类问题，将 \([CLS]\) token 输入分类器，用交</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] TIED: A Cycle Consistent Encoder-Decoder Model (CVPRW 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn63/</link>
      <pubDate>Thu, 29 Jul 2021 11:27:32 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn63/</guid>
      <description>TIED: A Cycle Consistent Encoder-Decoder Model for Text-to-Image Retrieval (CVPRW 2021) 概览 Natural Language (NL) based Vehicle Track Retrieval 任务，对时间也有要求。 本文提出一种文本到图像的编码器解码器网络 (Text-to-Image Encoder-Decoder network, TIED)，将图文映射到潜在空间</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 联邦学习 / 跨模态检索] FedCMR: Federated Cross-Modal Retrieval (SIGIR 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn62/</link>
      <pubDate>Wed, 28 Jul 2021 14:36:28 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn62/</guid>
      <description>FedCMR: Federated Cross-Modal Retrieval (SIGIR 2021) 概述 基于深度学习的方法需要大量高质量的多模态数据，而现实中，多模态数据由许多不同用户 (client) 分别生成。 本文研究联邦跨模态检索 (Federated Cross-Modal Retrieval, Fe</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ViT / 跨模态检索] Revamping Cross-Modal Recipe Retrieval (CVPR 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn61/</link>
      <pubDate>Wed, 28 Jul 2021 10:41:00 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn61/</guid>
      <description>Revamping Cross-Modal Recipe Retrieval with Hierarchical Transformers and Self-supervised Learning (CVPR 2021) 开源代码传送门 图像编码器 \(\phi_{img}\) ResNet-50 / ResNeXt / ViT 菜谱编码器 \(\phi_{rec}\) 三类数据要处理 标题 成分 指导 用三个独立的基于 Transformer 的编码器分别处理三种数据</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ViT] Early Convolutions Help Transformers See Better (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn60/</link>
      <pubDate>Tue, 27 Jul 2021 11:07:09 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn60/</guid>
      <description>2106.14881 Early Convolutions Help Transformers See Better (2021) ViT 模型对超参数敏感，不好优化；而 CNN 更容易优化。 本文认为问题在于 ViT 的前期视觉处理 (early visual processing)，通常是 patchify stem，</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索 / 图网络] Fine-grained Video-Text Retrieval with HGR (CVPR 2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn59/</link>
      <pubDate>Mon, 26 Jul 2021 11:53:00 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn59/</guid>
      <description>Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning (CVPR 2020) 开源代码传送门 核心 提出一种层级式的图推理模型 (Hierarchical Graph Reasoning model, HGR)，将视频-文本匹配分解为三级语义： 全局事件，在文本中对应整个句</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索 / 图网络] A Deep Local and Global Scene-Graph Matching (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn58/</link>
      <pubDate>Sun, 25 Jul 2021 11:30:31 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn58/</guid>
      <description>2106.02400 A Deep Local and Global Scene-Graph Matching for Image-Text Retrieval (2021) 核心 提出一种局部与全局场景图匹配 (Local and Global Scene Graph Matching, LGSGM) 方法。 视觉图编码器 文本编码器 图嵌入 用多尺度节点注意力将图嵌入为向量。 附</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ViT / ReID] TransReID: Transformer-based Object Re-Identification (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn57/</link>
      <pubDate>Wed, 21 Jul 2021 10:10:54 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn57/</guid>
      <description>2102.04378v2 TransReID: Transformer-based Object Re-Identification (2021) 开源代码传送门 背景 目标 ReID 尚未很好解决的两个问题 从全局视角提取丰富的结构模式 包含细节信息的细粒度特征提取受到下采样的限制 本文提出一</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ViT] CMT: Convolutional Neural Networks Meet Vision Transformers (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn56/</link>
      <pubDate>Tue, 20 Jul 2021 14:32:30 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn56/</guid>
      <description>2107.06263 CMT: Convolutional Neural Networks Meet Vision Transformers (2021) 核心 本文设计了一种 CNN 与 transformer 的交集，即 CMT 架构。 CMT 块 Local Perception Unit (LPU) 绝对的位置编码破坏了平移不变性，忽视了 patch 中的局部关联与结构信息。 $$LPU(X) =</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ViT] Incorporating Convolution Designs into Visual Transformers (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn55/</link>
      <pubDate>Tue, 20 Jul 2021 10:06:20 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn55/</guid>
      <description>2103.11816 Incorporating Convolution Designs into Visual Transformers (2021) 核心 设计一种通过卷积增强的图像 Transformer (Convolution-enhanced image Transformer, CeiT)，将 CNN 提取低级特征、强化局部性与 Transformer 提取长程依赖的优势相结合。 Image-to-Tokens 模块 $$x&#39; = I2T(x) = MaxPool(BN(Conv(x))).$$</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] Heterogeneous Attention Network (SIGIR 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn54/</link>
      <pubDate>Sun, 18 Jul 2021 19:39:50 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn54/</guid>
      <description>Heterogeneous Attention Network for Effective and Efficient Cross-modal Retrieval (SIGIR 2021) 背景 联合嵌入 (joint embedding) 方法的问题 只进行全局匹配 文本与图像只在匹配阶段有交互 多模态的 Transformer 系方法能够在较早的阶段实现跨模态交互，但</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索 / 图网络] Cross-Graph Attention Model (SIGIR 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn53/</link>
      <pubDate>Sun, 18 Jul 2021 11:25:44 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn53/</guid>
      <description>Cross-Graph Attention Enhanced Multi-Modal Correlation Learning for Fine-Grained Image-Text Retrieval (SIGIR 2021) 背景 SIGIR 2021 短文。 现有三类跨模态检索方法 全局相关性学习 局部相关性学习 高阶语义概念学习 在模态特定的语义概念 (modality-specific semantic concepts) 之外，还应</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索 / 图网络] Graph Structured Network (CVPR 2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn52/</link>
      <pubDate>Sun, 18 Jul 2021 10:10:07 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn52/</guid>
      <description>2004.00277 Graph Structured Network for Image-Text Matching (CVPR 2020) 开源代码传送门 总览 本文提出一种图结构匹配网络 (Graph Structured Matching Network, GSMN)，对目标、关系与属性显式建模。 以下两组相关性互相促进 细粒度目</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 频域 / ViT] M2TR: Multi-modal Multi-scale Transformers (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn51/</link>
      <pubDate>Sat, 17 Jul 2021 14:34:02 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn51/</guid>
      <description>2104.09770 M2TR: Multi-modal Multi-scale Transformers for Deepfake Detection (2021) 背景 Deepfake 检测任务。 本文提出一种多模态多尺度 Transformer (Multi-modal Multi-scale Transformer, M2TR)，包含一个多尺度 Transformer 模块 (MT) 和一个跨模态融合模块 (CMF)，利用频域</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 频域] Frequency learning for image classification (2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn50/</link>
      <pubDate>Fri, 16 Jul 2021 21:04:14 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn50/</guid>
      <description>2006.15476 Frequency learning for image classification (2020) 核心问题 如果一个神经网络完全设计成在频域进行操作会怎么样？ 本文设计了 FreqNet 研究上述问题。 本文方法 图像分块 对图像层级式地分块以提取多</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 频域] Global Filter Networks for Image Classification (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn49/</link>
      <pubDate>Fri, 16 Jul 2021 19:50:39 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn49/</guid>
      <description>2107.00645 Global Filter Networks for Image Classification (2021) 开源代码传送门 核心方法 本文提出一种全局滤波器网络 (Global Filter Network, GFNet)，在频域中学习空间位置之间的相互关系。 不同于视觉 transformer 中的自注</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 频域] Improving Image Classification With Frequency Domain Layers (MLSP 2017)</title>
      <link>http://jonathanwayy.xyz/2021/prn48/</link>
      <pubDate>Fri, 16 Jul 2021 15:02:02 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn48/</guid>
      <description>Improving image classification with frequency domain layers for feature extraction (MLSP 2017) 核心 研究从频域提取的特征对深度网络架构的作用。 提出频率特征提取层。 方法 对输入图像层级式分块以提取多粒度信息，对各块使</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 频域] High-Frequency Component Explain Generalization of CNNs (CVPR 2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn47/</link>
      <pubDate>Fri, 16 Jul 2021 10:29:32 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn47/</guid>
      <description>High-Frequency Component Helps Explain the Generalization of Convolutional Neural Networks (CVPR 2020) 背景 从数据视角研究 CNN 的泛化表现。 CNN 挖掘高频成分 将原始数据分解为高低频成分，\(x = \{x_{l}, x_{h}\}\)，分别简写为 LFC</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 频域 / 人脸伪造检测] Spatial-Phase Shallow Learning (CVPR 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn46/</link>
      <pubDate>Thu, 15 Jul 2021 21:12:47 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn46/</guid>
      <description>Spatial-Phase Shallow Learning: Rethinking Face Forgery Detection in Frequency Domain (CVPR 2021) 背景 人脸伪造检测 (face forgery detection) 任务。 在合成假脸过程中使用上采样，该操作通常会在频域留下痕迹。 核心方法与观点 本文提出一种空间-</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] Multi-Modality Cross Attention Network (CVPR 2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn45/</link>
      <pubDate>Wed, 14 Jul 2021 10:42:26 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn45/</guid>
      <description>Multi-Modality Cross Attention Network for Image and Sentence Matching (CVPR 2020) 出发点 既考虑模态间关联，也考虑模态内关联。 提出多模态交叉注意力网络 (Multi-Modality Cross Attention Network)，主要由自注意力模块与交叉注意</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 图像去噪] NBNet: Noise Basis Learning with Subspace Projection (CVPR 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn44/</link>
      <pubDate>Mon, 12 Jul 2021 21:56:05 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn44/</guid>
      <description>2012.15028 NBNet: Noise Basis Learning for Image Denoising with Subspace Projection (CVPR 2021) 核心思想 通过图像投影，利用非局部的图像信息。 由输入图像生成一组图像基向量 (image basis vectors)，接着在这些基向量张成</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 注意力机制] Non-Local Neural Networks (CVPR 2018)</title>
      <link>http://jonathanwayy.xyz/2021/prn43/</link>
      <pubDate>Mon, 12 Jul 2021 15:09:26 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn43/</guid>
      <description>Non-Local Neural Networks (CVPR 2018) 开源代码传送门 核心内容 提出非局部操作用于捕捉长距离依赖关系，通过输入特征图所有位置上特征的加权和计算各位置的响应值 (respons</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] CAMP: Cross-Modal Adaptive Message Passing (ICCV 2019)</title>
      <link>http://jonathanwayy.xyz/2021/prn42/</link>
      <pubDate>Mon, 12 Jul 2021 10:58:13 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn42/</guid>
      <description>CAMP: Cross-Modal Adaptive Message Passing for Text-Image Retrieval (ICCV 2019) 开源代码传送门 背景 现有方法的问题 现有方法通常学习一个公共的嵌入空间，在其中衡量特征相似度，使用 ranking loss 进行训练。 这类方法没有</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 频域 / 风格迁移] Photorealistic Style Transfer via Wavelet Transforms (ICCV 2019)</title>
      <link>http://jonathanwayy.xyz/2021/prn41/</link>
      <pubDate>Mon, 12 Jul 2021 10:12:48 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn41/</guid>
      <description>Photorealistic Style Transfer via Wavelet Transforms (ICCV 2019) 开源代码传送门 背景 风格迁移任务。 模型应当在不损伤图像细节的情况下实现风格迁移。 本文提出一种基于白化与色彩变换的小波矫正迁移 (wavelet</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 注意力机制] SA-Net: Shuffle Attention for Deep CNNs (ICASSP 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn40/</link>
      <pubDate>Sun, 11 Jul 2021 10:53:48 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn40/</guid>
      <description>2102.00240v1 SA-Net: Shuffle Attention for Deep Convolutional Neural Networks (ICASSP 2021) 开源代码传送门 背景 设计了 Shuffle Attention (SA) 模块，将特征沿着通道维分组，对每个子特征用 Shuffle 单元同时计算通道注意力与空间注意力。 Shuffle Attention (SA) 特</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 频域 / 图像恢复] Multi-level Wavelet-CNN for Image Restoration (CVPR 2018)</title>
      <link>http://jonathanwayy.xyz/2021/prn39/</link>
      <pubDate>Sat, 10 Jul 2021 16:41:48 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn39/</guid>
      <description>Multi-level Wavelet-CNN for Image Restoration (CVPR 2018) 背景 图像恢复 (Image Restoration) 的目的 从观测到的较差图像 \(y\) 恢复潜在的干净图像 \(x\)。 本文设计了一种多级小波 CNN (multi-level wavelet CNN, MWCNN) 模型，增大感受野，改善</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 频域 / Transformer] FNet: Mixing Tokens with Fourier Transforms (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn38/</link>
      <pubDate>Sat, 10 Jul 2021 15:34:14 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn38/</guid>
      <description>2105.03824 FNet: Mixing Tokens with Fourier Transforms (2021) 开源代码传送门 出发点 用更简单的 token 混合机制取代自注意力层。 最终选择傅利叶变换，设计了 FNet 模型。 离散傅利叶变换 (Discrete Fourier Transform, DFT) 傅利叶变换将</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ViT / 跨模态检索] Fine-grained Visual Textual Alignment (2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn37/</link>
      <pubDate>Sat, 10 Jul 2021 11:14:29 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn37/</guid>
      <description>2008.05231 Fine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders (2020) 背景 在分类任务上预训练的 CNN 网络所提取的特征通常只能捕捉到图像的全局描述，而忽视了重要的局部细节。 现有方法的问题 由于交</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 多模态预训练] Unicoder-VL (AAAI 2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn36/</link>
      <pubDate>Sat, 10 Jul 2021 10:00:50 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn36/</guid>
      <description>Unicoder-VL: A Universal Encoder for Vision and Language by Cross-Modal Pre-Training (AAAI 2020) 背景 尚未出现能够够直接处理跨模态任务与数据的预训练模型。 本文基于多层 Transformer 提出一种通用视觉语言编码器 (Universal Encoder for Vision And Language, Uni</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ViT] Swin Transformer (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn35/</link>
      <pubDate>Fri, 09 Jul 2021 10:54:19 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn35/</guid>
      <description>2103.14030 Swin Transformer: Hierarchical Vision Transformer using Shifted Windows (2021) 开源代码传送门 背景 Transformer 在视觉任务上的主要困难 视觉元素的尺度可能相当不同，但是当前工作中 token 都固定尺度 图像具有更高的像素分辨率</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 对抗样本] Generating Adversarial Examples with Adversarial Networks (IJCAI 2018)</title>
      <link>http://jonathanwayy.xyz/2021/prn34/</link>
      <pubDate>Wed, 07 Jul 2021 11:06:29 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn34/</guid>
      <description>Generating Adversarial Examples with Adversarial Networks (IJCAI 2018) AdvGAN 模型架构 $$\mathcal{L}_{GAN} = \mathbb{E}_{x} log \mathcal{D}(x) + \mathbb{E}_{x} log(1 - \mathcal{D}(x + \mathcal{G}(x))),$$ $$\mathcal{L}_{adv}^{f} = \mathbb{E}_{x}\mathcal{l}_f(x + \mathcal{G}(x), t),$$ $$\mathcal{L}_{hinge} = \mathbb{E}_{x} max(0, ||\mathcal{G}(x)||_{2} - c),$$ $$\mathcal{L} = \mathcal{L}_{adv}^{f} + \alpha \mathcal{L}_{GAN} + \beta \mathcal{L}_{hinge}.$$ 样本可视化</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 对抗样本] Query Attack via Opposite-Direction Feature (2018)</title>
      <link>http://jonathanwayy.xyz/2021/prn33/</link>
      <pubDate>Tue, 06 Jul 2021 15:13:34 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn33/</guid>
      <description>1809.02681 Query Attack via Opposite-Direction Feature:Towards Robust Image Retrieval (2018) 背景 现有分类攻击方法在在检索场景中的困难 其目标是类别预测，与检索任务不同 检索场景中训练时的类别与测试时通常是不同的 本文针</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- ViT] XCiT: Cross-Covariance Image Transformers (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn32/</link>
      <pubDate>Mon, 05 Jul 2021 12:59:51 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn32/</guid>
      <description>2106.09681 XCiT: Cross-Covariance Image Transformers (2021) 开源代码传送门 背景 Transformers 中自注意力模块计算复杂度高。 本文用一种转置的注意力 (transposed attention) 取代自注意力，称为交叉协方差注意力 (cross-covariance attention, XCA)，其对于</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索 / 图网络] Similarity Reasoning and Filtration (AAAI 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn31/</link>
      <pubDate>Sat, 03 Jul 2021 21:00:55 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn31/</guid>
      <description>2101.01368 Similarity Reasoning and Filtration for Image-Text Matching (AAAI 2021) 开源代码传送门 先前方法的缺陷 在局部特征之间计算基于标量的余弦相似度，可能并不足以表征区域与单词之间的关联模式 大多数方法使</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 注意力机制] VOLO: Vision Outlooker for Visual Recognition (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn30/</link>
      <pubDate>Sat, 03 Jul 2021 16:23:02 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn30/</guid>
      <description>2106.13112v2 VOLO: Vision Outlooker for Visual Recognition (2021) 背景 制约 ViT 不如 CNN 的一个主要因素 ViT 在将细粒度特征以及上下文编码成 token 时效率较低。 本文设计了一种简单轻量的注意力机制，称为 Outl</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 对抗样本 / ReID] Vulnerability of Person Re-Identification Models (CVPR 2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn29/</link>
      <pubDate>Fri, 02 Jul 2021 11:05:32 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn29/</guid>
      <description>Vulnerability of Person Re-Identification Models to Metric Adversarial Attacks (CVPR 2020) 背景 闭集 (closed-set) 任务，即训练与测试使用同样类别的任务上对抗样本已经有了较广泛的研究，开集 (open-set) 任务如 ReID 上的相关研究较少。 为了骗过</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 遮挡 ReID] High-Order Information Matters (CVPR 2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn28/</link>
      <pubDate>Thu, 01 Jul 2021 13:01:57 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn28/</guid>
      <description>High-Order Information Matters: Learning Relation and Topology for Occluded Person Re-Identification (CVPR 2020) 背景 本文研究遮挡 ReID 问题 (Occluded Person Re-Identification)，该问题主要受到遮挡 (occlusion) 和出界 (outliers) 两个问题困扰。大部分现</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 文本对抗样本] Seq2Sick: Evaluating Robustness of Seq2Seq Models (AAAI 2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn27/</link>
      <pubDate>Tue, 29 Jun 2021 21:36:09 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn27/</guid>
      <description>Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples (AAAI 2020) 开源代码传送门 背景 对抗攻击可用于衡量 DNN 的鲁棒性，对抗样本越容易生成则模型越健壮。 攻击图像比攻击文本容易得多，因为图像</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 频域 / 图网络] Beyond Low-frequency Information in GCNs (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn26/</link>
      <pubDate>Tue, 29 Jun 2021 12:00:44 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn26/</guid>
      <description>2101.00797 Beyond Low-frequency Information in Graph Convolutional Networks 背景 当前一些研究认为平滑信号，即低频信息是 GNN 成功的关键。 本文重点思考是否低频信息就能完全满足需求，以及其他信息在 GNN 中扮演着怎</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 频域 / 图网络] Spectral Graph Attention Network (2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn25/</link>
      <pubDate>Mon, 28 Jun 2021 19:34:36 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn25/</guid>
      <description>2003.07450 Spectral Graph Attention Network (2020) 背景 图注意力网络 (Graph Attention Network, GAT) 通过引入注意力机制优化 GCN 的卷积过程。具体来说，在节点聚合 (node aggregation) 阶段，GAT 赋予各边一个自注意力权重，用于捕</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 带噪跨模态检索] Learning Cross-Modal Retrieval With Noisy Labels (CVPR 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn24/</link>
      <pubDate>Mon, 28 Jun 2021 11:45:52 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn24/</guid>
      <description>Learning Cross-Modal Retrieval With Noisy Labels (CVPR 2021) 背景 为了应对较高的数据标注成本，大规模数据会用到一些非专业的标注资源，从而不可避免地在标签中引入了噪音信息。 由图 2 可以看出，</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 对抗样本] Cross-Modal Learning with Adversarial Samples (NeurIPS 2019)</title>
      <link>http://jonathanwayy.xyz/2021/prn23/</link>
      <pubDate>Sun, 27 Jun 2021 13:29:36 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn23/</guid>
      <description>Cross-Modal Learning with Adversarial Samples (NeurIPS 2019) 文中主要以跨模态哈希检索为例，其搜索空间大致可分为四个部分：T2T、I2I、I2T/T2I、NR (not relevant)。 理想的针</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 对抗样本] AI-GAN: Attack-Inspired Generation of Adversarial Examples (2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn22/</link>
      <pubDate>Sat, 26 Jun 2021 15:10:01 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn22/</guid>
      <description>2002.02196 AI-GAN: Attack-Inspired Generation of Adversarial Examples (2020) 本文设计了一种新的 GAN 的变体，称为 Attack-Inspired GAN (AI-GAN) 用于生成对抗扰动。 AI-GAN 的训练包含两个阶段： 第一阶段，联合训练一个生成器、一个鉴别器和一个</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 频域 / 迁移学习] FSDR: Frequency Space Domain Randomization (CVPR 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn21/</link>
      <pubDate>Sat, 26 Jun 2021 11:27:54 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn21/</guid>
      <description>FSDR: Frequency Space Domain Randomization for Domain Generalization (CVPR 2021) 背景 语义分割受限于数据标注的难度，因此带有自动生成标签的合成图像成了缓解这一问题的一个选择。 但是这类模型由于域偏置与偏移</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 频域] Learning in the Frequency Domain (CVPR 2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn20/</link>
      <pubDate>Fri, 25 Jun 2021 15:44:51 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn20/</guid>
      <description>Learning in the Frequency Domain (CVPR 2020) 出发点 受计算资源与内存限制，大多数 CNN 模型只用低分辨率的 RGB 图像作为输入，处理现实中高分辨率图像时要先缩小尺寸，而这一过程难免带来</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态哈希] Graph Convolutional Network Hashing (IJCAI 2019)</title>
      <link>http://jonathanwayy.xyz/2021/prn19/</link>
      <pubDate>Thu, 24 Jun 2021 15:26:11 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn19/</guid>
      <description>Graph Convolutional Network Hashing for Cross-Modal Retrieval (IJCAI 2019) 本文提出一种针对跨模态检索的图卷积网络哈希 (graph convolution network hashing, GCH)，由一个语义编码器、两个特征编码网络和一个基于融合模块的图卷积网</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] Cross-modal Scene Graph Matching (WACV 2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn18/</link>
      <pubDate>Wed, 23 Jun 2021 09:36:01 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn18/</guid>
      <description>Cross-modal Scene Graph Matching for Relationship-aware Image-Text Retrieval (WACV 2020) 出发点 正确的匹配除了要包含相同的目标以外，目标之间的关系也应当相同。 因而，本文使用视觉场景图 (visual scene graph, VSG) 和文本场景图 (textual scene graph, TSG)</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] Cross-Modal Center Loss (CVPR 2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn17/</link>
      <pubDate>Tue, 22 Jun 2021 19:53:32 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn17/</guid>
      <description>Cross-Modal Center Loss for 3D Cross-Modal Retrieval (CVPR 2021) 现有方法的问题 核心思想是最小化由预训练网络提取的多模态特征之间的跨模态差异，而这些预训练网络应当与跨模态数据联合训练 现有的</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态哈希] Deep Cross-Modal Hashing (CVPR 2017)</title>
      <link>http://jonathanwayy.xyz/2021/prn16/</link>
      <pubDate>Tue, 22 Jun 2021 16:33:39 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn16/</guid>
      <description>Deep Cross-Modal Hashing (CVPR 2017) 哈希的目标 将原始空间数据点映射为汉明空间中的二进制编码，在汉明空间中保留原始空间中的相似度。 两类多模态哈希 (Multi-Modal Hashing, MMH) 多源哈希 (multi-source hashing, MSH) 目的</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态哈希] Self-Supervised Adversarial Hashing Networks (CVPR 2018)</title>
      <link>http://jonathanwayy.xyz/2021/prn15/</link>
      <pubDate>Tue, 22 Jun 2021 10:54:41 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn15/</guid>
      <description>Self-Supervised Adversarial Hashing Networks for Cross-Modal Retrieval (CVPR 2018) 当前(当时)跨模态哈希方法的主要不足 直接使用单类标签来衡量跨模态的语义关联，而事实上标准的跨模态数据集中一个图像实例往往能</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] AXM-Net: Cross-Modal Context Sharing Attention Network (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn14/</link>
      <pubDate>Mon, 21 Jun 2021 14:24:22 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn14/</guid>
      <description>2101.08238 AXM-Net: Cross-Modal Context Sharing Attention Network for Person Re-ID (2021) 主要困难 各模态中与行人相关的信息结构相当不同 关键在于学习一个能够从数据中提取语义的网络，而不是在训练过程中简单记住各行</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 频域] FcaNet: Frequency Channel Attention Networks (2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn13/</link>
      <pubDate>Sun, 20 Jun 2021 14:25:10 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn13/</guid>
      <description>2012.11879 FcaNet: Frequency Channel Attention Networks (2020) 利用频率分析重新思索通道注意力，并从数学上证明传统的全局平均池化 (GAP) 是频域中特征分解的一种特殊情况。 GAP 的潜在问题 尽管简洁高效， GAP</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] Deep Adversarial Graph Attention Convolution Network (MM 2019)</title>
      <link>http://jonathanwayy.xyz/2021/prn12/</link>
      <pubDate>Thu, 17 Jun 2021 20:35:36 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn12/</guid>
      <description>Deep Adversarial Graph Attention Convolution Network for Text-Based Person Search (MM 2019) 先前工作的问题 孤立对待图像中的局部块，只考虑文本描述中单词级别的上下文关联，因而忽略了图文所包含的结构化语义信息 (structured semantic</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] TIPCB: A Simple but Effective Part-based Convolutional Baseline</title>
      <link>http://jonathanwayy.xyz/2021/prn11/</link>
      <pubDate>Tue, 15 Jun 2021 13:42:56 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn11/</guid>
      <description>2105.11628 TIPCB: A Simple but Effective Part-based Convolutional Baseline for Text-based Person Search 视觉特征学习 在视觉 CNN 分支，ResNet-50 第 3 和第 4 个残差块的输出分别作为低级特征图和高级特征图。 用 GMP 聚合低级特</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- VQA] Reasoning with Heterogeneous Graph Alignment (AAAI 2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn10/</link>
      <pubDate>Thu, 03 Jun 2021 18:30:46 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn10/</guid>
      <description>Reasoning with Heterogeneous Graph Alignment for Video Question Answering 出发点 需要一个统一的方法同步对模态间和模态内的关联性进行建模与推理。 本文所提到的 “视频段 (video shot)” 指的是一小段能被 3D 卷</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- VQA] Object-Centric Representation Learning (2021)</title>
      <link>http://jonathanwayy.xyz/2021/prn9/</link>
      <pubDate>Tue, 01 Jun 2021 09:48:18 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn9/</guid>
      <description>2104.05166 Object-Centric Representation Learning for Video Question Answering 模型高训练度带来的问题 这类模型更倾向于捕捉浅层模式 (shallow patterns)，因而会在浅层统计量形成捷径 (creating shortcuts through surface statistic</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- VQA] Location-Aware Graph Convolutional Networks (AAAI 2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn8/</link>
      <pubDate>Sun, 30 May 2021 14:39:03 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn8/</guid>
      <description>Location-aware graph convolutional networks for video question answering (AAAI 2020) 与 IQA 相比 VQA 的几个困难 由于视频包含大量帧，其视觉内容更为复杂，尤其是其中一些帧主要被与问题无关的背景内容 (strong background content) 占据 视频通常</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- VQA] Beyond RNNs: Positional Self-Attention with Co-Attention(AAAI 2019)</title>
      <link>http://jonathanwayy.xyz/2021/prn7/</link>
      <pubDate>Sat, 29 May 2021 14:57:33 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn7/</guid>
      <description>Beyond RNNs: Positional Self-Attention with Co-Attention for Video Question Answering (AAAI 2019) RNN + Attention 方法问题 耗时 由于 RNN 的特点，难以建模长距离依赖关系 本文提出了一个名为 Positional Self-Attention with Co-attention (PSAC) 的新架构，是首个无需使用 RNN 的模型。</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] Consensus-Aware Visual-Semantic Embedding (ECCV 2020)</title>
      <link>http://jonathanwayy.xyz/2021/prn6/</link>
      <pubDate>Sat, 29 May 2021 10:55:05 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn6/</guid>
      <description>2007.08883 Consensus-Aware Visual-Semantic Embedding for Image-Text Matching (ECCV 2020) 当前主流方法 将图像与文本投影到一个公共空间，通常无法充分利用图像中目标以及句子段之间的关系 分块级别的匹配 (fragment-level matching) + 聚合其相似度</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- VQA] Learnable Aggregating Net with Diversity Learning (MM 2019)</title>
      <link>http://jonathanwayy.xyz/2021/prn5/</link>
      <pubDate>Fri, 28 May 2021 19:43:09 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn5/</guid>
      <description>Learnable Aggregating Net with Diversity Learning for Video Question Answering (MM 2019) V-VQA 三个难点 视频通常包含大量冗余信息 一些视频相关问题涉及多个关键帧，较难定位 有效聚合视频与句子特征以捕捉回答真实分布的</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- VQA] Structured Two-stream Attention Network (AAAI 2019)</title>
      <link>http://jonathanwayy.xyz/2021/prn4/</link>
      <pubDate>Fri, 28 May 2021 15:21:20 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn4/</guid>
      <description>Structured two-stream attention network for video question answering (AAAI 2019) 图像 QA 中两种注意力机制 visual attention: &amp;ldquo;where to look&amp;rdquo; question attention: &amp;ldquo;what words to listen to&amp;rdquo; 视频 QA 三个主要困难 考虑长距离时域结构，同时不遗漏重要信息 为了定位相关视频实</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- VQA] Motion-Appearance Co-Memory Networks (CVPR 2018)</title>
      <link>http://jonathanwayy.xyz/2021/prn3/</link>
      <pubDate>Fri, 28 May 2021 14:06:27 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn3/</guid>
      <description>Motion-Appearance Co-Memory Networks for Video Question Answering (CVPR 2018) 视频 QA 与 图像 QA 相比三个独有特性 处理较长的图像序列，包含更丰富的信息（数量上及多样性上） 动作与外观信息通常互相关联，能够彼此</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- VQA] Gradually Refined Attention (MM 2017)</title>
      <link>http://jonathanwayy.xyz/2021/prn2/</link>
      <pubDate>Thu, 27 May 2021 13:55:03 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn2/</guid>
      <description>Video Question Answering via Gradually Refined Attention over Appearance and Motion (MM17) 延伸模型的缺陷 由 video captioning 与 ImageQA 等任务延伸而来的模型容易弱化或忽视视频的时域信息 这些模型将整个问题编码为单一特征，不具有足够</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- VQA] TGIF-QA (CVPR 2017)</title>
      <link>http://jonathanwayy.xyz/2021/prn1/</link>
      <pubDate>Thu, 27 May 2021 13:49:47 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2021/prn1/</guid>
      <description>Tgif-qa: Toward spatio-temporal reasoning in visual question answering (CVPR 2017) 开源代码传送门 三点重要贡献 提出专为视频 VQA 设计的三种新任务，需要对视频的时空推断(spatio-temporal reaso</description>
    </item>
    
  </channel>
</rss>

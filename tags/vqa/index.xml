<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>vqa on Zijie Wang`s Blog</title>
    <link>http://wzj.life/tags/vqa/</link>
    <description>Recent content in vqa on Zijie Wang`s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>All rights reserved - 2020</copyright>
    <lastBuildDate>Mon, 12 Jul 2021 10:58:13 +0800</lastBuildDate><atom:link href="http://wzj.life/tags/vqa/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[论文阅读笔记 -- 跨模态检索] CAMP: Cross-Modal Adaptive Message Passing (ICCV 2019)</title>
      <link>http://wzj.life/2021/prn42/</link>
      <pubDate>Mon, 12 Jul 2021 10:58:13 +0800</pubDate>
      
      <guid>http://wzj.life/2021/prn42/</guid>
      <description>CAMP: Cross-Modal Adaptive Message Passing for Text-Image Retrieval (ICCV 2019) 开源代码传送门 背景 现有方法的问题 现有方法通常学习一个公共的嵌入空间，在其中衡量特征相似度，使用 ranking loss 进行训练。 这类方法没有</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- VQA] Reasoning with Heterogeneous Graph Alignment (AAAI 2020)</title>
      <link>http://wzj.life/2021/prn10/</link>
      <pubDate>Thu, 03 Jun 2021 18:30:46 +0800</pubDate>
      
      <guid>http://wzj.life/2021/prn10/</guid>
      <description>Reasoning with Heterogeneous Graph Alignment for Video Question Answering 出发点 需要一个统一的方法同步对模态间和模态内的关联性进行建模与推理。 本文所提到的 “视频段 (video shot)” 指的是一小段能被 3D 卷</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- VQA] Object-Centric Representation Learning (2021)</title>
      <link>http://wzj.life/2021/prn9/</link>
      <pubDate>Tue, 01 Jun 2021 09:48:18 +0800</pubDate>
      
      <guid>http://wzj.life/2021/prn9/</guid>
      <description>2104.05166 Object-Centric Representation Learning for Video Question Answering 模型高训练度带来的问题 这类模型更倾向于捕捉浅层模式 (shallow patterns)，因而会在浅层统计量形成捷径 (creating shortcuts through surface statistic</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- VQA] Location-Aware Graph Convolutional Networks (AAAI 2020)</title>
      <link>http://wzj.life/2021/prn8/</link>
      <pubDate>Sun, 30 May 2021 14:39:03 +0800</pubDate>
      
      <guid>http://wzj.life/2021/prn8/</guid>
      <description>Location-aware graph convolutional networks for video question answering (AAAI 2020) 与 IQA 相比 VQA 的几个困难 由于视频包含大量帧，其视觉内容更为复杂，尤其是其中一些帧主要被与问题无关的背景内容 (strong background content) 占据 视频通常</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- VQA] Beyond RNNs: Positional Self-Attention with Co-Attention(AAAI 2019)</title>
      <link>http://wzj.life/2021/prn7/</link>
      <pubDate>Sat, 29 May 2021 14:57:33 +0800</pubDate>
      
      <guid>http://wzj.life/2021/prn7/</guid>
      <description>Beyond RNNs: Positional Self-Attention with Co-Attention for Video Question Answering (AAAI 2019) RNN + Attention 方法问题 耗时 由于 RNN 的特点，难以建模长距离依赖关系 本文提出了一个名为 Positional Self-Attention with Co-attention (PSAC) 的新架构，是首个无需使用 RNN 的模型。</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- VQA] Learnable Aggregating Net with Diversity Learning (MM 2019)</title>
      <link>http://wzj.life/2021/prn5/</link>
      <pubDate>Fri, 28 May 2021 19:43:09 +0800</pubDate>
      
      <guid>http://wzj.life/2021/prn5/</guid>
      <description>Learnable Aggregating Net with Diversity Learning for Video Question Answering (MM 2019) V-VQA 三个难点 视频通常包含大量冗余信息 一些视频相关问题涉及多个关键帧，较难定位 有效聚合视频与句子特征以捕捉回答真实分布的</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- VQA] Structured Two-stream Attention Network (AAAI 2019)</title>
      <link>http://wzj.life/2021/prn4/</link>
      <pubDate>Fri, 28 May 2021 15:21:20 +0800</pubDate>
      
      <guid>http://wzj.life/2021/prn4/</guid>
      <description>Structured two-stream attention network for video question answering (AAAI 2019) 图像 QA 中两种注意力机制 visual attention: &amp;ldquo;where to look&amp;rdquo; question attention: &amp;ldquo;what words to listen to&amp;rdquo; 视频 QA 三个主要困难 考虑长距离时域结构，同时不遗漏重要信息 为了定位相关视频实</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- VQA] Motion-Appearance Co-Memory Networks (CVPR 2018)</title>
      <link>http://wzj.life/2021/prn3/</link>
      <pubDate>Fri, 28 May 2021 14:06:27 +0800</pubDate>
      
      <guid>http://wzj.life/2021/prn3/</guid>
      <description>Motion-Appearance Co-Memory Networks for Video Question Answering (CVPR 2018) 视频 QA 与 图像 QA 相比三个独有特性 处理较长的图像序列，包含更丰富的信息（数量上及多样性上） 动作与外观信息通常互相关联，能够彼此</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- VQA] Gradually Refined Attention (MM 2017)</title>
      <link>http://wzj.life/2021/prn2/</link>
      <pubDate>Thu, 27 May 2021 13:55:03 +0800</pubDate>
      
      <guid>http://wzj.life/2021/prn2/</guid>
      <description>Video Question Answering via Gradually Refined Attention over Appearance and Motion (MM17) 延伸模型的缺陷 由 video captioning 与 ImageQA 等任务延伸而来的模型容易弱化或忽视视频的时域信息 这些模型将整个问题编码为单一特征，不具有足够</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- VQA] TGIF-QA (CVPR 2017)</title>
      <link>http://wzj.life/2021/prn1/</link>
      <pubDate>Thu, 27 May 2021 13:49:47 +0800</pubDate>
      
      <guid>http://wzj.life/2021/prn1/</guid>
      <description>Tgif-qa: Toward spatio-temporal reasoning in visual question answering (CVPR 2017) 开源代码传送门 三点重要贡献 提出专为视频 VQA 设计的三种新任务，需要对视频的时空推断(spatio-temporal reaso</description>
    </item>
    
  </channel>
</rss>

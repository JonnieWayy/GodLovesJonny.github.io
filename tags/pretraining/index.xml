<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>pretraining on Jonathan`s Blog</title>
    <link>http://jonathanwayy.xyz/tags/pretraining/</link>
    <description>Recent content in pretraining on Jonathan`s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>All rights reserved - 2020</copyright>
    <lastBuildDate>Sun, 24 Apr 2022 16:44:47 +0800</lastBuildDate><atom:link href="http://jonathanwayy.xyz/tags/pretraining/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[论文阅读笔记 -- 跨模态预训练] LiT: Zero-Shot Transfer with Locked-image text Tuning (2022)</title>
      <link>http://jonathanwayy.xyz/2022/prn222/</link>
      <pubDate>Sun, 24 Apr 2022 16:44:47 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2022/prn222/</guid>
      <description>2111.07991 LiT: Zero-Shot Transfer with Locked-image text Tuning (2022) 概述 零射迁移 (zero-shot transfer)，不同于传统零射学习，其预训练过程中看不到迁移相关的监督信息。 本文提出采用对比学习架构，提</description>
    </item>
    
    <item>
      <title>[论文阅读笔记 -- 跨模态预训练] Democratizing Contrastive Language-Image Pre-training (2022)</title>
      <link>http://jonathanwayy.xyz/2022/prn221/</link>
      <pubDate>Sat, 16 Apr 2022 20:27:36 +0800</pubDate>
      
      <guid>http://jonathanwayy.xyz/2022/prn221/</guid>
      <description>2203.05796 Democratizing Contrastive Language-Image Pre-training: A CLIP Benchmark of Data, Model, and Supervision (2022) 开源代码传送门 概述 现有关于 CLIP 的工作由于其训练策略及所用数据的差异，难以公平对比其表现。 本文旨在构建一个公平且可以</description>
    </item>
    
  </channel>
</rss>

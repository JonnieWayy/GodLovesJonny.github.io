<!DOCTYPE html>
<html lang="zh-cn">
  <head>
  
  <link rel="stylesheet" href="/js/katex/katex.min.css" >
  <script src="/js/katex/katex.min.js" > </script>
  <script src="/js/katex/contrib/auto-render.min.js" ></script>
  <script>
      document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body);
      });
  </script>

  <meta http-equiv="content-type" content="text/html;charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="robots" content="noodp"/>
  <meta name="author" content="Jonathan Wang">
  
  
  
  <link rel="prev" href="http://jonathanwayy.xyz/2021/prn31/" />
  <link rel="next" href="http://jonathanwayy.xyz/2021/ldp8/" />
  <link rel="canonical" href="http://jonathanwayy.xyz/2021/prn32/" />
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <title>
       
       
           [论文阅读笔记 -- ViT] XCiT: Cross-Covariance Image Transformers (2021) | Jonathan`s Blog
       
  </title>
  <meta name="title" content="[论文阅读笔记 -- ViT] XCiT: Cross-Covariance Image Transformers (2021) | Jonathan`s Blog">
    
  
  <link rel="stylesheet" href="/font/iconfont.css">
  <link rel="stylesheet" href="/css/main.min.css">


  
  
  
 

<script type="application/ld+json">
 "@context" : "http://schema.org",
    "@type" : "BlogPosting",
    "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "http:\/\/jonathanwayy.xyz"
    },
    "articleSection" : "posts",
    "name" : "[论文阅读笔记 -- ViT] XCiT: Cross-Covariance Image Transformers (2021)",
    "headline" : "[论文阅读笔记 -- ViT] XCiT: Cross-Covariance Image Transformers (2021)",
    "description" : "2106.09681 XCiT: Cross-Covariance Image Transformers (2021) 开源代码传送门 背景 Transformers 中自注意力模块计算复杂度高。 本文用一种转置的注意力 (transposed attention) 取代自注意力，称为交叉协方差注意力 (cross-covariance attention, XCA)，其对于",
    "inLanguage" : "zh-cn",
    "author" : "Jonathan Wang",
    "creator" : "Jonathan Wang",
    "publisher": "Jonathan Wang",
    "accountablePerson" : "Jonathan Wang",
    "copyrightHolder" : "Jonathan Wang",
    "copyrightYear" : "2021",
    "datePublished": "2021-07-05 12:59:51 \u002b0800 CST",
    "dateModified" : "2021-07-05 12:59:51 \u002b0800 CST",
    "url" : "http:\/\/jonathanwayy.xyz\/2021\/prn32\/",
    "wordCount" : "657",
    "keywords" : [ "paper reading","cv","notes","attention","transformer","ViT", "Jonathan`s Blog"]
}
</script>

  
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</head>

  


  <body class="">
    <div class="wrapper">
        <nav class="navbar">
    
        <div class="top-scroll-bar"></div>
    
    <div class="container">
        <div class="navbar-header header-logo">
            
            <span class="logo_mark" >></span>
            <a href="http://jonathanwayy.xyz">
                <span class="logo_text" >$ cd /home/ </span>
                <span class="logo_cursor" ></span>
            </a>
            
        </div>
        
            
            
        
        <div class="navbar-right">
                
                <span class="menu">
                
                <a class="menu-item" href="/posts/" title="">Blog</a>
                
                <a class="menu-item" href="/categories/" title="">Categories</a>
                
                <a class="menu-item" href="/tags/" title="">Tags</a>
                
                <a class="menu-item" href="/about/" title="">About</a>
                
                </span>
                <a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-dark-mode"></i></a>
        </div>
    </div>
</nav>
<nav class="navbar-mobile" id="nav-mobile" style="display: none">
     
         <div class="top-scroll-bar"></div>
     
     <div class="container">
        <div class="navbar-header">
            <div class="header-logo">
                
                    <span class="logo_mark">></span>
                    <a href="http://jonathanwayy.xyz">
                        <span class="logo_text">$ cd /home/ </span>
                        <span class="logo_cursor"></span>
                </a>
                
            </div>
            <div class="navbar-right">
                <a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-dark-mode"></i></a>
                <div class="menu-toggle">
                    <span></span><span></span><span></span>
                </div>
            </div>
        </div>
     
          <div class="menu" id="mobile-menu">
                
                
                <a class="menu-item" href="/posts/" title="">Blog</a>
                
                <a class="menu-item" href="/categories/" title="">Categories</a>
                
                <a class="menu-item" href="/tags/" title="">Tags</a>
                
                <a class="menu-item" href="/about/" title="">About</a>
                
        </div>
    </div>
</nav>

    	 <main class="main">
          <div class="container">
      		
<article class="post-warp" itemscope itemtype="http://schema.org/Article">
    <header class="post-header">
        <h1 class="post-title" itemprop="name headline">[论文阅读笔记 -- ViT] XCiT: Cross-Covariance Image Transformers (2021)</h1>
        <div class="post-meta">
                
                Written by <a itemprop="name" href="http://jonathanwayy.xyz" rel="author">Jonathan Wang</a> 
                <span class="post-time">
                on <time datetime=2021-07-05 itemprop="datePublished">July 5, 2021</time>
                </span>
                in
                <i class="iconfont icon-folder"></i>
                <span class="post-category">
                        <a href="http://jonathanwayy.xyz/categories/paper-reading-notes/"> Paper Reading Notes </a>
                        
                </span>
                <span class="post-word-count">, 657 words</span>
        </div>
    </header>
    <div class="post-content">
        

        
            
        

        
        
     
          
          
          

          
          
          

          <h1 id="210609681-xcit-cross-covariance-image-transformers-2021">2106.09681 XCiT: Cross-Covariance Image Transformers (2021)</h1>
<p><a href="https://github.com/facebookresearch/xcit">开源代码传送门</a></p>
<p><img src="/images/2021/PRN32/2.png" alt="Fig 2"></p>
<h2 id="背景">背景</h2>
<p>Transformers 中自注意力模块计算复杂度高。</p>
<p>本文用一种转置的注意力 (transposed attention) 取代自注意力，称为交叉协方差注意力 (cross-covariance attention, XCA)，其对于 patch 数具有线性的复杂度，进而构建了交叉协方差图像 Transformers (Cross-Covariance Image Transformers, XCiT)。</p>
<p><img src="/images/2021/PRN32/1.png" alt="Fig 1"></p>
<h2 id="gram-矩阵与协方差矩阵的关系">Gram 矩阵与协方差矩阵的关系</h2>
<p>未归一化的 \(d \times d\) 协方差矩阵为 \(C = X^TX\)，\(N \times N\) 的 Gram 矩阵为 \(G = XX^T\)。</p>
<p>二者特征谱 (sigenspectrum) 中的非零部分是等价的，其特征向量可以互相求得。若 \(G\) 的特征向量为 \(V\)，则 \(C\) 的特征向量为 \(U=XV\)。</p>
<p>利用这一联系，本文试图避免计算 \(N \times N\) 注意力矩阵所带来的平方复杂度，该矩阵由 \(G\) 得到：</p>
<p>$$QK^T = XW_{q}W_{k}^TX^T.$$</p>
<p>本文考虑使用 \(d_{k} \times d_{q}\) 的 \(C\)：</p>
<p>$$K^TQ = W_{k}^TX^TXW_{q}.$$</p>
<h2 id="交叉协方差注意力-cross-covariance-attention">交叉协方差注意力 (Cross-Covariance Attention)</h2>
<p>沿着特征维操作，而非 token 维。</p>
<p>$$XCAttention(Q, K, V) = V\mathcal{A}_{XC}(K, Q),$$
$$\mathcal{A}_{XC}(K, Q) = Softmax(\hat{K}^T\hat{Q} / \tau).$$</p>
<p>对 query 和 key 矩阵的各列取 \(\mathcal{l}_{2}\) 范数，能够提高训练的稳定性，但是由于限制了自由度会降低该操作的表征能力，因而引入一个可学习的 temperature 参数 \(\tau\)。</p>
<h3 id="对角线分块的交叉协方差注意力-block-diagonal-cross-covariance-attention">对角线分块的交叉协方差注意力 (Block-diagonal Cross-Covariance Attention)</h3>
<p>将特征分组，与多头思路类似。对各组分别使用该注意力，学习独立的权重矩阵。</p>
<h4 id="两个好处">两个好处</h4>
<ul>
<li>聚合 value 的复杂度进一步降低</li>
<li>更容易优化，并带来结果上的提升</li>
</ul>
<p>与 Group Normalization 类似。</p>
<h4 id="可视化呈现">可视化呈现</h4>
<p><img src="/images/2021/PRN32/3.png" alt="Fig 3"></p>
<h2 id="cross-covariance-image-transformers">Cross-covariance Image Transformers</h2>
<h3 id="local-patch-interaction-lpi-block">Local Patch Interaction (LPI) Block</h3>
<p>在每个 XCA 块后加上 LPI 块，使得 patch 之间有显式的信息交互，由两个面向深度 (depth-wise) 的 \(3 \times 3\) 卷积层中间加 BN + GELU 构成。</p>
<h3 id="前向传播">前向传播</h3>
<p>Point-wise feed-forward network (FFN)，是单个包含 \(4d\) 个隐节点的隐藏层。</p>
<p>XCA 块中特征在各组内交互优化，LPI 块因为面向深度，其中没有特征交互，FFN 使得所有特征之间能够进行交互。</p>
<h3 id="通过类注意力进行全局聚合">通过类注意力进行全局聚合</h3>
<p>训练针对图像分类的模型时使用类注意力层 (class attention layers)。</p>

    </div>

    <div class="post-copyright">
             
            <p class="copyright-item">
                <span>Author:</span>
                <span>Jonathan Wang </span>
                </p>
            
           
             
            <p class="copyright-item">
                    <span>Link:</span>
                    <a href=http://jonathanwayy.xyz/2021/prn32/>http://jonathanwayy.xyz/2021/prn32/</span>
            </p>
            <p>
                <span>Ads:</span>
                <span>欢迎参与<a href="https://www.didiyun.com" target="_blank" style="text-decoration:underline">滴滴云AI大师活动</a>购买 GPU / vGPU / 机器学习产品，使用我的滴滴AI大师码 <b style="color:red">0724</b> 可享受 <b>9 折</b>优惠！</span>
            </p>
            
             
            <p class="copyright-item lincese">
                本文采用<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可
            </p>
            
    </div>

  
    <div class="post-tags">
        
            <section>
            <i class="iconfont icon-tag"></i>Tag(s): 
            
            <span class="tag"><a href="http://jonathanwayy.xyz/tags/paper-reading/">
                    #paper reading</a></span>
            
            <span class="tag"><a href="http://jonathanwayy.xyz/tags/cv/">
                    #cv</a></span>
            
            <span class="tag"><a href="http://jonathanwayy.xyz/tags/notes/">
                    #notes</a></span>
            
            <span class="tag"><a href="http://jonathanwayy.xyz/tags/attention/">
                    #attention</a></span>
            
            <span class="tag"><a href="http://jonathanwayy.xyz/tags/transformer/">
                    #transformer</a></span>
            
            <span class="tag"><a href="http://jonathanwayy.xyz/tags/vit/">
                    #ViT</a></span>
            
            </section>
        
        <section>
                <a href="javascript:window.history.back();">back</a></span> · 
                <span><a href="http://jonathanwayy.xyz">home</a></span>
        </section>
    </div>

    <div class="post-nav">
        
        <a href="http://jonathanwayy.xyz/2021/prn31/" class="prev" rel="prev" title="[论文阅读笔记 -- 跨模态检索 / 图网络] Similarity Reasoning and Filtration (AAAI 2021)"><i class="iconfont icon-left"></i>&nbsp;[论文阅读笔记 -- 跨模态检索 / 图网络] Similarity Reasoning and Filtration (AAAI 2021)</a>
         
        
        <a href="http://jonathanwayy.xyz/2021/ldp8/" class="next" rel="next" title="炼丹杂记 -- 时域抽样定理">炼丹杂记 -- 时域抽样定理&nbsp;<i class="iconfont icon-right"></i></a>
        
    </div>

    <div class="post-comment">
          
                 
          
    </div>
    <h5 id="wc" style="font-size: 1rem;text-align: center;">700 Words|Read in about 2 Min|本文总阅读量<span id="busuanzi_value_page_pv"></span>次</h5>
</article>
          </div>
		   </main>
      <footer class="footer">
    <div class="copyright">
        &copy;
        
        <span itemprop="copyrightYear">2020 - 2023</span>
        
        <span class="with-love">
    	 <i class="iconfont icon-love"></i> 
         </span>
         
            <span class="author" itemprop="copyrightHolder"><a href="http://jonathanwayy.xyz">Jonathan Wang</a> | </span> 
         

         
		  <span>Powered by <a href="https://gohugo.io/" target="_blank" rel="external nofollow">Hugo</a> & <a href="https://github.com/liuzc/leaveit" target="_blank" rel="external nofollow">LeaveIt</a></span> 
          <span id="busuanzi_container_site_pv">
              本站访问量：<span id="busuanzi_value_site_pv"></span>次
          </span>
    </div>
</footer>













    
     <link href="//lib.baomitu.com/lightgallery/1.6.11/css/lightgallery.min.css" rel="stylesheet">  
      
     <script src="/js/vendor_gallery.min.js" async="" ></script>
    
  






     </div>
  </body>
</html>
